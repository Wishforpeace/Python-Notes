# 计算思维复习

## 1.计算思维

### 计算机的概念

计算机是==数据处理模型==

> 它根据事先存储在存储器中的一组指令的要求，接收输入的数据、处理输入的数据、 并且输出数据处理结果。

数学家**冯·诺依曼**最早提出计算机制造的**冯·诺依曼模型**

+ 采用二进制逻辑
+ 程序存储执行
+ 计算机核心组件
  + ==运算器==
  + ==控制器==
  + ==存储器==

### 计算机硬件

计算机系统中由电子、机械和光电元件等组成的各种物理装置的总称

+ CPU
+ 主板
+ 内存条
+ 各种输入输出设备
  + 键盘
  + 鼠标

### 数据

计算机处理数据仅包含0和1，将各种信息转换都需要转换成0和1，这个过程叫==编码==

### 计算机软件

+ 系统软件
+ 应用软件

### 计算思维实例

#### 阿基米德的割圆术

#### 刘徽的割圆术

### 计算机的发展历史

+ **帕斯卡**
+ **莱布尼兹**
+ Charles Babbage(1792-1871)
  + Difference  Engine & Analytical Engine

+ Jacquard Loom

  + **提花织机**（Jacquard Loom）

  + > 它利用穿孔卡片（类似于存储程序）来控制织布过程中经线的提升。

+ Herman Hollerith ==美国信息处理之父==
  + 卡片制表机（Tabulating Machine）

### 早期电子计算机

#### Colossus（巨人计算机）

**阿兰**·**图灵**

1936年发表的论文，提出计算机抽象模型

1943年，设计了Colossus，主要用于破解德国密码

#### ENIAC

第一台电子计算机ENIAC诞生于1946年，由==莫奇利与埃克特设计==

编程通过插孔和开关实现，计算速度为5000次/秒，占地1000平方英尺

### 现代计算机的诞生

**第一代计算机（大约1950-1959年）**
**第二代计算机（大约1959-1965年）**
**第三代计算机（大约1965-1975年）**
**第四代计算机（大约1975-1985年）**
**第五代计算机（大约1985-现在）**

## 2.定位数系

### 数系分类

+ 简单分群数系

  + > 简单分群数系的记载出现在人类早期文明的遗迹中，古埃及人、古巴比伦人、古希腊雅典人以及古罗马人就是使用的简单分群数系。
    > 古埃及人与古巴比伦人使用的都是象形文字，古埃及人使用形似一根竖线的符号|表示单个数量，这和其他一些文明中表示单个数量的数字符号不谋而合，比如前面提到的古罗马人使用字母I表示单个数量，中国古代大约是春秋战国时期开始作为计算工具使用的算筹，就是用一根小木棍表示单个数量。 

+ 乘法分群数系

  + > 可以对简单分群数系在数的表示上做些改进，对十以内的九个自然数（没有零）分别使用不同的单个符号表示，而不是只使用一个符号|，使用阿拉伯数字1-9表示十以内的自然数。 
    > A、B、C等表示更大自然数的符号依然保留，对应不同自然数的符号只出现一次，而不是像简单分群数系里面那样会出现多个相同符号罗列的情况，比如简单分群数系表示的数BBAAA||||使用乘法分群数系表示为2B3A4，每个大写英文字母前面的数字表示该字母的个数。

+ 定位数系

  + > 定位数系可以看做是在乘法分群数系的基础上再次改进，以乘法分群数系表示的自然数3B2A5为例，数字3表示B的个数，数字2表示A的个数，而且从小到大的数量单位1、A、B、C等都是按照从右到左的顺序排列，何不干脆将数量单位去掉，数量单位前面的数字采取固定位置摆放。
    >
    >具体来说右边第一位对应十以内数字（也就是数量单位1的个数），右边第二位固定为数量单位A的个数，右边第三位固定为数量单位B的个数，以此类推。
    >因此乘法分群数系表示的自然数3B2A5 ，转换为定位数系可表示为325，表示该数包含3个B、2个A和5个1。

### 二进制

### 八进制

### 十六进制

## 3.数据存储

>数据由一种格式转变成另一种格式的过程被称为编==码（encoding）==，使用阿拉伯数字表示的数与使用罗马数字表示的数，就是针对数值型数据的两种不同的编码形式。
>构成计算机存储系统最基本的物理元器件通常只有两种状态，使用这两种状态模拟数据的表示，最简单直接的方法就是将一种状态表示 0，另一种状态表示 1，多个物理元器件的排列就可以表示 0 和 1 组成的数字串，所以计算机存储设备可以存储的信息是 0 和 1 组成的数字串信息。
>计算机存储的信息中，单个数字 0 或 1 被称为位（binary digit，英文缩写bit），因为bit可读作比特，所以binary digit有时也被称为比特位，多个比特位组成的数字串被称为位模式（bit pattern），==8个比特位构成的位模式被称为字节（byte）==。

### 数值型数据在计算机中的存储

#### 整数的存储

+ ##### 无符号表示法

  + 存储到计算机中的整数，最简单的一种编码方式就是将该整数转换成二进制形式，不过**这种方法不能区分具有相同绝对值的正负整数**，也就是说这种编码形式适合于保存非负整数，因此这种编码方式也称为无符号表示法（unsigned representation）。

  + 将一个整数按照无符号表示法编码成n位位模式的方法是：将该数转换成二进制形式，若转换后数位不足n位，则在该数的左边补0凑足n位。
    反过来将一个使用n位无符号表示法表示的位模式还原为整数（解码）的方法是：将位模式最左边开始的连续若干个0舍去，然后将剩余的位模式（当做二进制整数）转换为十进制整数。
  + n位位模式可以有2n种不同的排列，共可以表示2n个非负整数，从小到大排列为0到2n−1，所以使用n位无符号表示法可以表示的最小整数是0，可以表示的最大整数是2n−1。
  + 大于2n−1的整数转换为二进制整数时数位会超出n位，此时选取最右边的n位位模式作为编码信息，那么左边若干位会被舍去，再反过来解码后得到的整数并不是原来的整数，这种因为超出存储限制导致的编码错误被称为==溢出（overflow）==。

+ 符号加绝对值表示法

  + 只能存储非负整数的计算机系统显然是功能不完整的，为了解决负整数的编码问题，可以考虑将符号位引入到编码后的位模式，也就是说将最左边的一位作为符号位，0表示非负整数，1表示负整数，其余的位模式作为存储这个数绝对值的数据位，这种编码方法被称为符号加绝对值表示法（sign-and-magnitude representation）。

    > 比如整数-7，如果还是用8位位模式来编码，就将编码后最左边的一位作为符号位，用1表示；然后将-7的绝对值7转换成二进制形式(111)2，除去最左边的符号位，还有7位用于保存(111)2，于是将111左边补4个0凑足7位得到0000111；再加上最左边的符号位1，最终-7的编码形式是==10000111== 。对于整数7，其编码后的形式是00000111。 

+ 二进制补码表示法
  + 修复方案1直接在符号加绝对值表示法的基础上仅仅修改一个位模式1000对应的整数，将其由原来对应的-0改为-8；
  + 修复方案2在修复方案1的基础上对1开头的位模式对应的整数在排序上做了个反序调整。（应用最广）

#### 小数的存储

##### （1）二进制数的规范化形式

二进制数的规范化形式是将二进制数表示成“==$\pm a.xx…x\times2^n$==”的形式（其中a是数字1，n是整数）。二进制小数变成规范化形式后由符号位（负还是非负）、小数（a.xx…x）和指数（$2^n$）这三部分组成。
二进制小数的存储涉及到这三部分的分别存储。符号位的存储需要保存的是该数“非负”还是“负”，所以仅需占用1个比特位足以；小数部分仅需要存储小数点后面的数字；指数部分仅需要存储指数n即可。指数n尽管是整数，但是存储指数采用的编码并不是二进制补码，而是余码。

##### （2）余码表示法

通过将原本大体上“正负各半”的整数集通过全体成员加上一个固定的正整数后，变成非负整数集，然后再转换成二进制数进行编码的方法就是余码表示法（excess representation）。
余码表示法中被加入的正整数被称为偏移量，编码位数不同，要表示的整数范围不同，使用的偏移量就不同。同样是4位编码长度，如果要使用余码表示法编码整数-7到8，则使用的偏移量是7。可以用符号excess_k表示偏移量为k的余码表示法。
比如-3使用4位excess_7编码的过程为：先将-3加上7得到4，然后将4转成二进制数得到100，左边补1个0得到最终编码形式0100。
再比如3使用4位excess_7编码的过程也是先将3加上7得到10，然后将10转成二进制数得到1010，这就是最终的编码形式。

##### （3）浮点表示法

计算机存储小数的方法是将该数使用规范化表示后，存储符号、尾数（小数点后面的数字）、指数这三部分信息，这种存储方法被称为浮点表示法（floating-point representation）。
浮点表示法的格式遵循美国电气和电子工程师协会（Institute of Electrical and Electronics Engineers，英文缩写为IEEE）提出的两种标准：一种是单精度（single）表示法，使用32位位模式；另一种是双精度（double）表示法，使用64位位模式。

![](https://github.com/Wishforpeace/Typora/blob/main/%E8%AE%A1%E7%AE%97%E6%80%9D%E7%BB%B4%EF%BC%88%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%89/%E6%B5%AE%E7%82%B9%E8%A1%A8%E7%A4%BA%E6%B3%95.png)

## 4.程序设计概念

### 计算机硬件的核心组成部分

+ CPU
+ Inout/Output
+ Memory

#### 中央处理单元(Central Processing Unit,CPU)![](https://github.com/Wishforpeace/Typora/blob/main/%E8%AE%A1%E7%AE%97%E6%80%9D%E7%BB%B4%EF%BC%88%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%89/image-20211227152147372.png)



ALU:算术逻辑单元，处理程序运行时的算术运算与逻辑运算

R：数据寄存器，保存程序运行时处理的运算数据

I：指令寄存器，保存程序运行时的每一条机器指令

PC：程序计数器，保存程序运行时的每一条机器指令在内存中的地址

Control Unit：控制单元，实现对CPU从内存中读取指令、翻译指令、执行指令的操作控制

#### 内存（Memory，有时称为主存）

![](https://github.com/Wishforpeace/Typora/blob/main/%E8%AE%A1%E7%AE%97%E6%80%9D%E7%BB%B4%EF%BC%88%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%89/image-20211227152531536.png)

#### 内存可以保存的数据形式为全部由0或1组成的位模式（0或1组成的数字串），8位构成1个字节，是最小的数据存储单位，每个字节会被分配一个编号（也就是常说的内存地址），在计算机系统中，内存地址使用二进制形式表示。

### 输入输出系统（Input Output System，简称IOS）

![](https://github.com/Wishforpeace/Typora/blob/main/%E8%AE%A1%E7%AE%97%E6%80%9D%E7%BB%B4%EF%BC%88%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%89/image-20211227152721690.png)

计算机系统最典型的输入设备包括键盘和鼠标，最典型的输出设备是显示器。其它常用设备如硬盘和光驱（较早计算机系统还有软驱）则既是输入设备也是输出设备。
程序在计算机上执行前先保存在硬盘（或U盘、光盘、软盘）中，程序开始执行时，操作系统会将程序装入内存中，保存到内存中的程序由若干条CPU可识别的机器指令构成，CPU中的控制单元从内存中==读取指令、翻译指令、执行指令==，然后重复这个过程直到程序执行结束。
CPU进行一次==读取指令、翻译指令、执行指令的过程被称为指令周期==，单位时间内执行的指令周期数量被称为==CPU的频率==。
程序在执行过程中有时需要使用程序的用户输入数据，这时用户通过输入设备将数据输入到内存中，然后CPU再从内存中读取数据进行后续操作，程序运行结果先保存到内存中，再由内存转移到输出设备上。

### 计算机程序设计语言的发展

+ #### 机器语言

  >机器语言是用二进制代码表示的计算机能直接识别和执行的一种机器指令的集合。它是计算机的设计者通过计算机的硬件结构赋予计算机的操作功能，机器语言编写的程序等于直接向CPU发指令， CPU可以直接处理和执行机器指令。

+ #### 汇编语言

  >为了减轻机器语言带给程序员的不适应，人们对机器语言进行了升级和改进：用一些容易理解和记忆的单词来代替特定的指令。通过这种方法，人们很容易去阅读程序或者理解程序正在执行的功能，对现有程序的修改和维护都变得更加简单方便，这种语言就是我们所说的汇编语言。
  >由于汇编语言使用了人类熟悉的助记符，计算机无法像机器语言编写的程序一样直接识别和执行，必须把汇编语言转换成能够被计算机识别和处理的机器语言。

+ #### 高级语言

  >使用机器语言和汇编语言进行程序设计时，要求使用者必须了解计算机的硬件结构，而不同型号的计算机硬件结构是不一样的，这极不利于程序在不同型号计算机上的推广。
  >随着计算机技术的发展，计算机科学家开发出接近自然语言且与具体硬件结构无关的程序设计语言，使用这些语言编写程序可以不需要了解计算机的硬件细节，这样的程序设计语言被称为高级程序设计语言，简称为高级语言。典型代表有：Fortran、Pascal、C、C++、Java、Python等。
  >高级语言也分为结构化程序设计语言与面向对象的程序设计语言， Fortran、Pascal、C 是结构化程序设计语言， C++、Java、Python 是面向对象的程序设计语言。

  #### 编译与解释

+ 高级语言编写的程序也被称为==源程序或源代码==，需要经过转换成==机器语言==编写的程序后才能被计算机执行，转换过程有**编译**与**解释**两种。

+ ==两者的区别在于==，

  + 编译是一次性将高级语言编写的程序全部转换成机器语言编写的程序后再执行，
  + 解释则是将高级语言编写的程序逐条转换成机器语言编写的程序，每转换一条后就执行一条。

实现**编译功能**的程序被称为==编译器==，实现**解释功能**的程序被称为==解释器==。
常用的高级语言中，C和C++是编译型语言，Python是解释性语言。Java即是编译性也是解释性语言。
源程序(或源代码)转换后的机器可执行的程序也被称为目标程序或目标代码，计算机可执行的程序保存在后缀名为“exe”的文件中，所以后缀名为“exe”的文件也被称为可执行文件。

## 5.算法简介

### 算法特征 

+ 有限性（有穷性）
+ 确定性
+ 输入
+ 输出
+ 能行性

### 机器学习算法简介

>机器学习的研究最早可以追溯到1952 年，IBM科学家Arthur Samuel设计了一款可以学习的西洋跳棋程序。它能通过观察棋子的走位来构建新的模型，并用其提高自己的下棋技巧。Samuel和这个程序进行多场对弈后发现，随着时间的推移程序的棋艺变得越来越好。Samuel用这个程序推翻了以往“机器无法超越人类，不能像人一样写代码和学习”这一传统认识，他对机器学习的定义是：==不需要确定性编程就可以赋予机器某项技能的研究领域。==

#### 机器学习算法分为两大类：

##### 监督学习：

> 监督学习是指学习数据本身被进行了**==人工标注==**，算法利用数据训练模型，然后用模型预测那些没有标注的数据，给出预测结果。例如，利用已经人工标注好猫和狗的图片各1000张，训练一个模型，可以让计算机识别没有见过的新图片，判断它是猫还是狗。

监督学习分为：

###### 回归问题：

> 回归问题指的是预测结果为==连续值==的问题，如线性回归、股票价格预测、天气预报等。

###### 分类问题：

>分类问题指的是预测结果为==离散值==的问题，如图片分类、人脸识别、神经网络分类等。目前流行的神经网络算法因其网络层次较多，也被称为==深度学习算法==。有一种解决分类问题的算法叫做==逻辑斯蒂回归算法==，虽然名字中有回归，它实际上是一种分类算法。

**监督学习算法使用人工标注好的数据得到模型的过程称为==训练==，用模型再去对没有人工标注的数据进行分类或回归值的过程称为==预测==。监督学习算法处理的数据又通常被分成==训练集和测试集==，==训练集==用于==构建==模型，==测试集==用于==测试==模型的效率。**

##### 无监督学习：

> 无监督学习是指学习数据本身是**==没有人工标注==**的，算法根据数据本身的特点给出预测结果。例如有猫狗图片各1000张，但并没有标明哪些是猫哪些是狗，让计算机自动把这些图片分成两类。

###### 聚类问题：

> 使用无监督学习算法解决的问题中最典型的是聚类问题，它可以简单地描述为：根据在数据中发现的描述对象及其关系的信息，将数据对象分组。下图是聚类问题的一个实例，左上角子图中黑点表示没有被人工标记的对象，其余三个子图为执行不同聚类算法之后的分类结果，相同颜色与形状的点被划分为一类。

![](https://github.com/Wishforpeace/Typora/blob/main/%E8%AE%A1%E7%AE%97%E6%80%9D%E7%BB%B4%EF%BC%88%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%89/%E5%9B%BE%E7%89%871.png)

### 查找算法

#### 顺序查找

